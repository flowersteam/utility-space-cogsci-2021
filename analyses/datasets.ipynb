{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T23:40:11.884989Z",
     "start_time": "2020-12-02T23:40:11.079428Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine raw data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Main data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Concatenate main raw data and codify participant IDs into a more readable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T23:43:19.913314Z",
     "start_time": "2020-12-02T23:43:18.051477Z"
    },
    "code_folding": [],
    "hidden": true,
    "jupyter": {
     "source_hidden": true
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving combined data to /Users/alexten/Projects/utility-space/data/combined_main2g.csv\n",
      "saving codes to /Users/alexten/Projects/utility-space/data/combined_main2g.csv\n"
     ]
    }
   ],
   "source": [
    "def combine_main_raw(input_data_paths, save_path, save_codes=''):\n",
    "    combine_list = []\n",
    "    cdicts = []\n",
    "    for input_data_path in input_data_paths:\n",
    "        df = pd.read_csv(input_data_path)\n",
    "        \n",
    "        # Codify subject IDs\n",
    "        uniqids = df.sid.astype('category')\n",
    "        df.loc[:, 'sid'] = uniqids.cat.codes\n",
    "        cdict = dict(enumerate(uniqids.cat.categories))\n",
    "        \n",
    "        # If not the 1st DF, continue enumerating from previous DF's last index\n",
    "        if combine_list:\n",
    "            last_index = combine_list[-1].loc[:, 'sid'].max()\n",
    "            df.loc[:, 'sid'] += last_index + 1\n",
    "            cdict = dict(enumerate(uniqids.cat.categories, last_index + 1))\n",
    "            \n",
    "        cdicts.append(cdict)    \n",
    "        combine_list.append(df)\n",
    "        \n",
    "    # Combine dataframes\n",
    "    df = pd.concat(combine_list)\n",
    "    cdict = {k:v for d in cdicts for k, v in d.items()}\n",
    "    df_codes = pd.DataFrame({'code': list(cdict.keys()), 'uid': list(cdict.values())})\n",
    "\n",
    "    # Save combined data\n",
    "    if save_path:\n",
    "        print('saving combined data to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(path.join(save_path), index=False)\n",
    "        \n",
    "    if save_codes:\n",
    "        print('saving codes to {}'.format(path.abspath(save_path)))\n",
    "        df_codes.to_csv(path.join(save_codes), index=False)\n",
    "    \n",
    "combine_main_raw(\n",
    "    input_data_paths = ('../data/raw/ig_main.csv', '../data/raw/eg_main.csv'),#, 'data/raw/ignf_main.csv'),\n",
    "    save_path = '../data/combined_main2g.csv',\n",
    "    save_codes = '../data/uid_codes.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra data files (self-reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate extra raw data and codify participant IDs using codes from the previous function. Then convert data to long format and clean up for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T19:47:24.615883Z",
     "start_time": "2020-12-03T19:47:24.017953Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>group</th>\n",
       "      <th>activity</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A1</td>\n",
       "      <td>comp</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A2</td>\n",
       "      <td>comp</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A3</td>\n",
       "      <td>comp</td>\n",
       "      <td>7</td>\n",
       "      <td>2.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A4</td>\n",
       "      <td>comp</td>\n",
       "      <td>9</td>\n",
       "      <td>4.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A1</td>\n",
       "      <td>lrn1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.928571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sid  group activity  item  rating  rating_norm\n",
       "0    0      0       A1  comp       1    -3.928571\n",
       "1    0      0       A2  comp       3    -1.928571\n",
       "2    0      0       A3  comp       7     2.071429\n",
       "3    0      0       A4  comp       9     4.071429\n",
       "4    0      0       A1  lrn1       3    -1.928571"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving combined data to /Users/alexten/Projects/utility-space/data/combined_extra.csv\n"
     ]
    }
   ],
   "source": [
    "def combine_extra_raw(input_data_paths, main_data_path, codes_path, save_path=''):\n",
    "    # Combine data and use previously generated codes\n",
    "    df = pd.concat([pd.read_csv(p) for p in input_data_paths]).set_index('sid')\n",
    "    df = df.merge(\n",
    "        pd.read_csv(codes_path).rename(columns={'uid': 'sid'}).set_index('sid'), on='sid').reset_index()\n",
    "    df.loc[:, 'sid'] = df.code\n",
    "    df.drop(columns=['code', 'age', 'gender', 'race', 'ethnicity', 'thoughts', 'comments'], inplace=True)\n",
    "    \n",
    "    # Reformat column names to convert to long format\n",
    "    rename_dict = {}\n",
    "    stubnames = []\n",
    "    for i in range(len(df.columns)):\n",
    "        s = df.columns[i]\n",
    "        if '.' in s:\n",
    "            split_str = s.split('.')\n",
    "            suffix = split_str.pop()\n",
    "            stubname = ''.join(split_str)\n",
    "            rename_dict[s] = '%'.join([stubname, suffix])\n",
    "            stubnames.append(stubname)\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    \n",
    "    # Convert to long format\n",
    "    df = pd.melt(df, id_vars=['sid', 'group'], value_vars=None, var_name='item%family', value_name='rating')\n",
    "    split_cols = df.loc[:, 'item%family'].str.split('%',  expand = True)\n",
    "    split_cols.columns = ['item', 'family']\n",
    "    df = pd.concat([df, split_cols], axis=1).filter(items=['sid','group','family','item','rating'])\n",
    "    \n",
    "    # Identify activity type for each family\n",
    "    act_df = pd.read_csv(main_data_path).filter(items=['sid','family','activity']).drop_duplicates()\n",
    "    df = df.merge(act_df, on=['sid','family']).filter(items=['sid','group','activity','item','rating'])\n",
    "    \n",
    "    # Sort values and rename items\n",
    "    df = df.sort_values(by=['group','sid','item','activity']).reset_index(drop=True)\n",
    "    df.loc[:, 'item'] = df.item.replace({'futurelearn0': 'lrn1',\n",
    "                                         'futurelearn1': 'lrn2',\n",
    "                                         'interested': 'int',\n",
    "                                         'progress': 'prog',\n",
    "                                         'time': 'time',\n",
    "                                         'rule': 'rule',\n",
    "                                         'complex': 'comp'\n",
    "                                        })\n",
    "    \n",
    "    # Add normalized scores\n",
    "    mean_ratings = df.groupby(['sid']).mean().loc[:, 'rating'].reset_index().rename(columns={'rating':'norm'})\n",
    "    df = df.merge(mean_ratings, on='sid')\n",
    "    df.loc[:, 'rating_norm'] = df.rating - df.norm\n",
    "    df = df.drop(columns='norm')\n",
    "    display(df.head())\n",
    "    \n",
    "    # Save combined data\n",
    "    if save_path:\n",
    "        print('saving combined data to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(path.join(save_path), index=False)\n",
    "\n",
    "        \n",
    "\n",
    "combine_extra_raw(\n",
    "    input_data_paths = ('data/raw/ig_extra.csv', 'data/raw/eg_extra.csv', 'data/raw/ignf_extra.csv'),\n",
    "    main_data_path = 'data/combined_main.csv',\n",
    "    codes_path = 'data/uid_codes.csv',\n",
    "    save_path = 'data/combined_extra.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T14:17:09.510337Z",
     "start_time": "2020-10-25T14:17:09.505646Z"
    },
    "heading_collapsed": "true"
   },
   "source": [
    "# Exclude outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Exclude outliers based on allocation bias and response bias. Report number of exclusions in each group based on allocation bias, then exclude from remaining data according response bias and report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T17:44:03.837373Z",
     "start_time": "2020-11-20T17:43:50.570693Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786e9499deb04a48ad6d462ed54e113f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Progress: '), FloatProgress(value=0.0, max=499.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high_ab</th>\n",
       "      <th>high_rb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       high_ab  high_rb\n",
       "group                  \n",
       "0           35        5\n",
       "1           20        3\n",
       "2            6        6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75 outliers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>stage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th>activity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th>A1</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>A1</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
       "      <th>A1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sid  stage\n",
       "group activity            \n",
       "0     A1         18     18\n",
       "      A2          6      6\n",
       "      A3          6      6\n",
       "      A4          5      5\n",
       "1     A1         12     12\n",
       "      A2          3      3\n",
       "      A3          1      1\n",
       "      A4          4      4\n",
       "2     A1          1      1\n",
       "      A2          3      3\n",
       "      A3          1      1\n",
       "      A4          1      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "group\n",
       "0    159\n",
       "1    178\n",
       "2     87\n",
       "Name: sid, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_clean_dataset(input_data_path, save_path, **kwargs):\n",
    "    # Define a response bias function\n",
    "    def rbf(x):\n",
    "        _, response_counts = np.unique(x.response, return_counts=True)\n",
    "        return np.max(response_counts) / np.sum(response_counts)\n",
    "\n",
    "\n",
    "    # Open combined data file\n",
    "    df = pd.read_csv(input_data_path, index_col=None).set_index('sid')\n",
    "\n",
    "    # Initialize columns to record values of interest\n",
    "    df['alloc_bias'], df['resp_bias'] = 0, 0\n",
    "\n",
    "    # Calculate values of interest\n",
    "    activities = ('A1', 'A2', 'A3', 'A4')\n",
    "    for sid, sdf in tqdm(df.groupby(by='sid'), desc='Progress: '):\n",
    "        # Allocation variance\n",
    "        counts = [sum(sdf.activity == i) for i in activities]\n",
    "        allocation_variance = np.std(counts)\n",
    "        df.loc[sid, 'alloc_bias'] = allocation_variance\n",
    "\n",
    "        # Response bias\n",
    "        response_bias = sdf.groupby('family').apply(rbf).mean()\n",
    "        df.loc[sid, 'resp_bias'] = response_bias\n",
    "\n",
    "    # Detect high allocation variance and response bias\n",
    "    df_ = df.reset_index().groupby('sid').head(1).reset_index()\n",
    "    df_['high_ab'] = df_.alloc_bias >= kwargs['ab_crit']\n",
    "    df_['high_rb'] = np.logical_and(df_.resp_bias > df_.resp_bias.mean() + kwargs['rb_crit'] * df_.resp_bias.std(), ~df_.high_ab)\n",
    "\n",
    "    display(df_.groupby(by='group')[['high_ab', 'high_rb']].sum().astype(int))\n",
    "    print('Found {} outliers'.format(np.logical_or(df_.high_ab, df_.high_rb).sum()))\n",
    "\n",
    "    # Exclude outliers\n",
    "    outlier = df_.loc[df_.high_ab | df_.high_rb, 'sid']\n",
    "    odf = df.loc[df.index.isin(df_.loc[df_.high_ab, 'sid']), :]\n",
    "    odf = odf.groupby(['group','sid','activity'])[['stage']].count().reset_index()\n",
    "    odf = odf.sort_values(['group','sid','stage']).groupby(['group','sid']).tail(1)\n",
    "    display(odf.groupby(['group', 'activity']).count())\n",
    "    df = df.loc[~df.index.isin(outlier), :]\n",
    "    display(df.reset_index().groupby(by='group')['sid'].nunique())\n",
    "\n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.reset_index().to_csv(save_path, index=False)\n",
    "    \n",
    "\n",
    "make_clean_dataset(\n",
    "    input_data_path = '../data/combined_main.csv',\n",
    "    save_path = '',#'data/clean_data_.csv',\n",
    "\n",
    "    # Set outlier criteria\n",
    "    ab_crit = 100,   # allocation variance critical value\n",
    "    rb_crit = 2 ,    # response bias critical value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Time-window approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "|Heuristic|Description ($t_i$ = trial number $i$; $w$ = window size)|\n",
    "|:-------:|:--------------------------------------------------------|\n",
    "| **PC**  | overall competence ($t_0$ to $t_i$)                     |\n",
    "| **rPC** | recent competence ($t_{i-w}$ to $t_i$)                  |\n",
    "| **rLP** | recent learning progress ($t_{i-w}$ to $t_i$)           |\n",
    "| **SC**  | self-challenge                                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T15:16:11.992675Z",
     "start_time": "2020-12-02T15:13:48.601818Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82266f0bd4f94857a493fec66130b987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Progress'), FloatProgress(value=0.0, max=337.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to /Users/alexten/Projects/utility-space/data/behavior_data.csv\n"
     ]
    }
   ],
   "source": [
    "def make_behavior_dataset(input_data_path, save_path):\n",
    "    # Read clean data and drop unused data\n",
    "    df = pd.read_csv(input_data_path, index_col=None).set_index(['sid','activity'])\n",
    "    df = df.loc[:, 'group,stage,trial,correct'.split(',')]\n",
    "    df = df.loc[df.trial <= 60+250]\n",
    "    df = df.loc[df.group != 2, :]\n",
    "    df['score'] = df.correct.astype(int).replace(0, -1)\n",
    "    df = df.sort_index()\n",
    "    cols = ['sid','group','activity','n','nfree','alloc','ipc15','fpc15','pcall','fscore','maxscore','speed','hirun']\n",
    "    outdict = dict(zip(cols, [[] for col in cols]))\n",
    "    for i, sdf in tqdm(df.groupby('sid'), desc='Progress'):\n",
    "        for a in ['A1', 'A2', 'A3', 'A4']:\n",
    "            outdict['sid'].append(i)\n",
    "            outdict['group'].append(sdf.group.values[0])\n",
    "            outdict['activity'].append(a)\n",
    "            \n",
    "            x = sdf.loc[(i, a), 'correct'].values.astype(int)\n",
    "            y = sdf.loc[(i, a), 'score'].values.astype(int)\n",
    "            \n",
    "            outdict['n'].append(x.size)\n",
    "            outdict['nfree'].append(x.size-15)\n",
    "            outdict['alloc'].append((x.size-15)/250)\n",
    "            outdict['ipc15'].append(x[:15].mean()) # initial window pc\n",
    "            outdict['fpc15'].append(x[-15:].mean()) # final window pc\n",
    "            outdict['pcall'].append(x.mean()) # final overall pc\n",
    "            outdict['fscore'].append(max([y.sum(), 0]))\n",
    "            outdict['maxscore'].append(max([np.cumsum(y).max(), 0]))\n",
    "            outdict['speed'].append(max([np.cumsum(y).max(), 0])/(np.cumsum(y).argmax()+1))\n",
    "            outdict['hirun'].append(len(max(re.sub(' +', '%', ''.join(x.astype(str)).replace('0',' ')).split('%'))))\n",
    "            \n",
    "    df = pd.DataFrame(outdict)\n",
    "    display(df.speed.min())\n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "    \n",
    "make_behavior_dataset(\n",
    "    input_data_path = '../data/clean_data.csv',\n",
    "    save_path = '../data/behavior_data.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Q-learning and RPE approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T23:22:57.466617Z",
     "start_time": "2020-11-29T23:22:35.521082Z"
    },
    "hidden": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def q_func(outcomes, init, lrate):\n",
    "    opes = [outcomes[1] - init]\n",
    "    qs = [init + lrate*(outcomes[1] - init)]\n",
    "    for i, o in enumerate(outcomes[1:]):\n",
    "        ope = o - qs[i]\n",
    "        q = qs[i] + lrate*(ope)\n",
    "        qs.append(q)\n",
    "        opes.append(ope)\n",
    "    return np.array(qs), np.array(opes)\n",
    "\n",
    "\n",
    "def make_heuristics_dataset(input_data_path, save_path, init_q, lrate, ope_smoothing, **kwargs):\n",
    "    # Read clean data and drop unused data\n",
    "    df = pd.read_csv(input_data_path, index_col=None).set_index(['sid','activity'])\n",
    "    df = df.loc[:, 'group,stage,trial,correct'.split(',')]\n",
    "    df = df.loc[df.trial <= 60+250]\n",
    "\n",
    "    # Add new columns\n",
    "    activities = 'A1,A2,A3,A4'.split(',')\n",
    "    for heuristic in ['conf','ope']:\n",
    "        for a in activities:\n",
    "            df['{}{}'.format(heuristic, a[1])] = np.nan\n",
    "\n",
    "    # Calculate dynamic performance heuristics for each subject\n",
    "    act_codes = {'A1':1, 'A2':2, 'A3':3, 'A4':4}\n",
    "    for i, sdf in tqdm(df.groupby('sid'), desc='Progress'):\n",
    "        for a in activities:\n",
    "            x = sdf.loc[(i, a), 'correct'].astype(int)\n",
    "\n",
    "            # Compute confidence and outcome prediction errors using Q-updating\n",
    "            conf, opes = q_func(x.values.squeeze(), init_q, lrate)\n",
    "            df.loc[(i, a), 'conf{}'.format(a[1])] = conf\n",
    "\n",
    "            # Recent learning progress (rlp)\n",
    "            opes = np.abs(pd.Series(opes).rolling(window=ope_smoothing, min_periods=1).mean()).values\n",
    "            df.loc[(i, a), 'ope{}'.format(a[1])] = opes\n",
    "        \n",
    "        df.loc[(i, slice(None)), :] = df.loc[(i, slice(None)), :].fillna(method='ffill', axis=0)\n",
    "\n",
    "    df = df.reset_index().sort_values(by=['sid', 'trial'])\n",
    "    display(df.loc[(df.sid == 0) & (df.trial >= 1) & (df.trial < 70), :])    # Display data excerpt\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "    \n",
    "make_heuristics_dataset(\n",
    "    input_data_path = 'data/clean_data.csv',\n",
    "    save_path = 'data/heuristics_data_alt.csv',\n",
    "    init_q = .5,\n",
    "    lrate = .2,\n",
    "    ope_smoothing = 6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAM designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T19:23:50.960663Z",
     "start_time": "2020-11-19T19:23:49.869175Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "jupyter": {
     "source_hidden": true
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def get_mps(df, **kwargs):\n",
    "    '''Find mastery points'''\n",
    "    arr = df.values\n",
    "    mask = (arr != 0)\n",
    "    arr = np.where(mask.any(axis=0), mask.argmax(axis=0), kwargs['invalid_val'])\n",
    "    return pd.Series(arr, dtype=kwargs['dtype'])\n",
    "\n",
    "\n",
    "def make_nam_dataset(input_data_path, save_path, **kwargs):\n",
    "    # Load data\n",
    "    df = pd.read_csv(input_data_path, index_col='sid')\n",
    "\n",
    "    # Select free-play trials\n",
    "    df = df.loc[(df.trial <= 60+250) & (df.trial >= 60)]\n",
    "    df.loc[:, 'trial'] -= 60\n",
    "    \n",
    "    # Get group dataset\n",
    "    group_df = df.groupby('sid').head(1)[['group']]\n",
    "\n",
    "    # Evaluate each trial's recent PC to True if mastery criterion was reached\n",
    "    mastered = df.reset_index().set_index(['sid','trial']).loc[:, 'rpc1':'rpc3'] >= kwargs['crit']\n",
    "    \n",
    "    # For each subject, find mastery points and NAM\n",
    "    by_sid = mastered.groupby('sid')\n",
    "    mastery_points = by_sid.apply(get_mps, invalid_val=250, dtype='int')\n",
    "    mastery_points.rename(columns={0:'mp1', 1:'mp2', 2:'mp3'}, inplace=True)\n",
    "    nam = by_sid.any().sum(axis=1).to_frame(name='nam')\n",
    "\n",
    "    # Display output dataset excerpt\n",
    "    nam = group_df.merge(nam, on='sid')\n",
    "    nam_df = nam.merge(mastery_points, on='sid').reset_index()\n",
    "    display(nam_df.head(10))\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        nam_df.to_csv(save_path, index=False)    \n",
    "\n",
    "\n",
    "make_nam_dataset(\n",
    "    input_data_path = 'data/heuristics_data.csv', \n",
    "    save_path = 'data/nam_data.csv',\n",
    "    crit = 13/15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T13:52:11.946221Z",
     "start_time": "2020-11-27T13:52:03.649112Z"
    },
    "code_folding": [],
    "hidden": true,
    "jupyter": {
     "source_hidden": true
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def make_learning_dataset(heuristics_data_path, nam_data_path, save_path, **kwargs):    \n",
    "    # Load heuristics data\n",
    "    df = pd.read_csv(heuristics_data_path, index_col='sid')\n",
    "    \n",
    "    # Add NAM classification\n",
    "    df = df.merge(pd.read_csv(nam_data_path, index_col='sid').drop(columns='group'), on='sid')\n",
    "    \n",
    "    # Annotate switch trials\n",
    "    df['switch'] = 0\n",
    "    choices = df.activity.values\n",
    "    switches = df.switch.values.copy() \n",
    "    switches[1:] = choices[:-1] != choices[1:]\n",
    "    df.loc[:, 'switch'] = switches\n",
    "    df.loc[df.stage=='train', 'switch'] = 0  # changing activity during forced stage is not switching\n",
    "    df.loc[df.trial==61, 'switch'] = 0       # choosing activity for the first time is not switching\n",
    "    \n",
    "    # Select free-play trials\n",
    "    df = df.loc[(df.trial <= 60+250) & (df.trial > 60)]\n",
    "    df.loc[:, 'trial'] -= 61\n",
    "    \n",
    "    # For each subject, compute learning stats from free play stage\n",
    "    df.reset_index(inplace=True)\n",
    "    outdf = []\n",
    "    for i, sdf in tqdm(df.groupby('sid'), desc='Progress: '):\n",
    "        _sdf = sdf.set_index('trial') # index subject data by trial\n",
    "    \n",
    "        # Get subject information (group, nam, mps) as a pandas Series\n",
    "        profile = _sdf.head(1)[['group', 'nam', 'mp1', 'mp2', 'mp3']].iloc[0]\n",
    "\n",
    "        # Get intervals between consecutive mastery points\n",
    "        mps = profile['mp1':'mp3'].values\n",
    "        sorted_lep_bounds = np.sort(np.unique([0] + mps.tolist() + [250]))  \n",
    "        lep_intervals = pd.IntervalIndex.from_arrays(sorted_lep_bounds[:-1], sorted_lep_bounds[1:], closed='right')\n",
    "\n",
    "        # Get intervals between consecutive swiches\n",
    "        switch_trials = _sdf.switch.values.nonzero()[0].tolist() + [250]\n",
    "        if switch_trials[0] != 1: switch_trials.insert(0,1)\n",
    "        streaks = pd.IntervalIndex.from_arrays(switch_trials[:-1], switch_trials[1:], closed='right')\n",
    "\n",
    "        # Calculate self-challenge (SC) summaries\n",
    "        sc = _sdf.sc\n",
    "        sc_flat = np.mean(sc)\n",
    "        sc_lep = sc.groupby(pd.cut(_sdf.index.astype(int), lep_intervals)).mean().mean()\n",
    "        sc_streaks = sc.groupby(pd.cut(_sdf.index.astype(int), streaks)).mean().mean()    \n",
    "\n",
    "        # Calculate weighted initial (dwipc) and final (dwfpc) performances (+ flat performances)\n",
    "        dwipc = (_sdf.loc[0, 'rpc1':'rpc3'].values * kwargs['difficulty_weights']).sum()\n",
    "        dwfpc = (_sdf.loc[249, 'rpc1':'rpc3'].values * kwargs['difficulty_weights']).sum()\n",
    "        ipc = _sdf.loc[0, 'rpc1':'rpc3'].sum()/3\n",
    "        fpc = _sdf.loc[249, 'rpc1':'rpc3'].sum()/3\n",
    "        \n",
    "        # Get profile info and see if subject mastered activities in order of difficulty\n",
    "        sid = i\n",
    "        group = profile['group']\n",
    "        nam = profile['nam']\n",
    "        progressive = (np.diff(np.array([1,2,3])[np.argsort(mps)]) == 1).all()\n",
    "        \n",
    "        # Store subject's learning stats\n",
    "        outdf.append(\n",
    "            pd.Series(\n",
    "                data = [sid,group,nam,progressive,dwipc,dwfpc,ipc,fpc,sc_flat,sc_lep,sc_streaks], \n",
    "                index='sid,group,nam,progressive,dwipc,dwfpc,ipc,fpc,sc_flat,sc_lep,sc_streaks'.split(',')\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    outdf = pd.DataFrame(outdf).sort_values(by=['group','sid'])\n",
    "    display(outdf.head())\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        outdf.to_csv(save_path, index=False)\n",
    "    \n",
    "\n",
    "make_learning_dataset(\n",
    "    heuristics_data_path = 'data/heuristics_data.csv',\n",
    "    nam_data_path = 'data/nam_data.csv',\n",
    "    save_path = 'data/learning_data.csv',\n",
    "    difficulty_weights = np.array([1,2,3])/6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice-modeling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T15:16:57.501542Z",
     "start_time": "2020-12-02T15:16:49.316780Z"
    },
    "hidden": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def prep_modeling_data(heuristics_data_path, nam_data_path, save_path):\n",
    "    # Load data\n",
    "    df = pd.read_csv(heuristics_data_path, index_col='sid')\n",
    "    \n",
    "    # Combine with NAM dataset\n",
    "    df = df.merge(pd.read_csv(nam_data_path, index_col='sid').drop(columns='group'), on='sid')\n",
    "    \n",
    "    # Encode activity choices as one-hot vectors\n",
    "    activity_codes = df.activity.str.get(1)\n",
    "    df = pd.concat([df, pd.get_dummies(activity_codes, prefix='ch', prefix_sep='')], axis = 1)\n",
    "    \n",
    "    add_data, act_inds = [], ['1','2','3','4']\n",
    "    for i, sdf in tqdm(df.groupby('sid'), desc='Progress'):\n",
    "        # Get relative time data\n",
    "        trials_per_activity = sdf.loc[:, 'ch1':'ch4'].cumsum(axis=0)\n",
    "        trials_total = np.tile(np.arange(trials_per_activity.shape[0]) + 1, [4, 1]).T\n",
    "        relt = trials_per_activity / trials_total\n",
    "        relt.columns = ['relt' + i for i in act_inds]\n",
    "        \n",
    "        # Get absolute time data\n",
    "        abst = trials_per_activity\n",
    "        abst.columns = ['abst' + i for i in act_inds]\n",
    "        \n",
    "        # Get previous trial choice data\n",
    "        prev = sdf.loc[:, 'ch1':'ch4']\n",
    "        prev.iloc[1:, :] = prev.iloc[:-1, :]\n",
    "        prev.iloc[0, :] = np.nan\n",
    "        prev.columns = ['prev' + i for i in act_inds]\n",
    "        \n",
    "        # Store into list\n",
    "        add_data.append([relt, prev, abst])\n",
    "    \n",
    "    add_data = pd.concat([pd.concat(h, axis=0) for h in zip(*add_data)], axis=1)\n",
    "    df = pd.concat([df, add_data], axis=1).reset_index()\n",
    "\n",
    "    # Exclude training trials\n",
    "    df = df.loc[df.trial.gt(60), :]\n",
    "    df.loc[:, 'trial'] -= 60\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "\n",
    "prep_modeling_data(\n",
    "    heuristics_data_path = 'data/heuristics_data.csv',\n",
    "    nam_data_path = 'data/nam_data.csv',\n",
    "    save_path = 'data/model_data.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitted parameters dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T19:51:38.210706Z",
     "start_time": "2020-12-02T19:51:37.708731Z"
    },
    "hidden": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def prep_fitted_data(fitted_models_data_path, save_path=''):\n",
    "    # Load data\n",
    "    df = pd.read_csv(fitted_models_data_path).set_index(['sid', 'vars'])\n",
    "\n",
    "    # Initialize empty dict to turn into DF\n",
    "    var_names = df.index.get_level_values(1).to_series().unique()\n",
    "    var_names = max(list(var_names), key=len).split(',')\n",
    "    col_names = ['sid', 'vars'] + var_names + ['tau']\n",
    "    df_dict = dict(zip(col_names, [[] for _ in col_names]))\n",
    "    \n",
    "    # Iterate through DF and extract model params from stored csv strings\n",
    "    for i, row in df.iterrows():\n",
    "        if row.aic is np.nan: continue\n",
    "        df_dict['sid'].append(i[0])\n",
    "        df_dict['vars'].append(i[1])\n",
    "        params = [float(p) for p in row.params.split(',')]\n",
    "        df_dict['tau'].append(params.pop())\n",
    "        vars_included = i[1].split(',')\n",
    "        for vn in var_names:\n",
    "            df_dict[vn].append(params[vars_included.index(vn)] if vn in vars_included else np.nan)\n",
    "\n",
    "    # Generate DF from df_dict and merge with initial DF\n",
    "    df = df.filter(items=['group','nam','aic']).merge(\n",
    "        right = pd.DataFrame(df_dict).set_index(['sid', 'vars']),\n",
    "        on = ['sid', 'vars']\n",
    "    ).reset_index()\n",
    "    \n",
    "    display(df.head())\n",
    "\n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "\n",
    "prep_fitted_data(\n",
    "    fitted_models_data_path = 'data/model_results/param_fits_raw.csv',\n",
    "    save_path = 'data/model_results/param_fits_clean.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exposure-competence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T21:12:39.472822Z",
     "start_time": "2020-12-02T21:12:37.951504Z"
    },
    "hidden": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def prep_data(data_path, nam_data_path, save_path):\n",
    "    full_df = pd.read_csv(data_path, index_col='sid').filter(items=['activity', 'stage', 'correct'])\n",
    "    full_df = full_df.merge(pd.read_csv(nam_data_path, index_col='sid').filter(items=['nam']), on='sid')\n",
    "    display(full_df.head())\n",
    "    activities = ['A1','A2','A3','A4']\n",
    "    \n",
    "    out_dfs = []\n",
    "    for nam in [1,2,3]:\n",
    "        out_dict = dict(zip(activities, [[] for a in activities]))\n",
    "        df = full_df.loc[full_df.nam.eq(nam), :]\n",
    "        for i, sdf in df.groupby('sid'):\n",
    "            for act_ind, sub_sdf in sdf.groupby('activity'):\n",
    "                out_dict[act_ind].append(sub_sdf.loc[:, 'correct'].tolist())\n",
    "\n",
    "        for k, v in out_dict.items():\n",
    "            rect_array = lut.boolean_indexing(v, fillval=np.nan)\n",
    "            pc = np.nanmean(rect_array, axis=0)\n",
    "            data_size = pc.size\n",
    "            pad_size = 310 - data_size\n",
    "            pc = np.concatenate([pc, np.full(pad_size, np.nan)])\n",
    "            pc[data_size:] = pc[data_size-50:data_size].mean()\n",
    "            out_dict[k] = pc\n",
    "\n",
    "        df = pd.DataFrame(out_dict).fillna(method='ffill')\n",
    "        df['nam'] = nam\n",
    "        out_dfs.append(df)\n",
    "    \n",
    "    df = pd.concat(out_dfs, axis=0).filter(items=['nam']+activities)\n",
    "    display(df.head())\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "    \n",
    "with np.errstate(all='ignore'):\n",
    "    prep_data(\n",
    "        data_path = 'data/clean_data.csv',\n",
    "        nam_data_path = 'data/nam_data.csv',\n",
    "        save_path = 'data/exposure_data.csv'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
